# QAFIM
Glaucoma, as one of the main causes of blindness, is of crucial importance for early diagnosis due to its insidious progression of onset. Existing deep-learning methods face the challenges of large image size and high cost of ROI annotation in fundus image classification. This paper proposes a Quantum-inspired Anchor Feature Interaction Mechanism (QAFIM) for end-to-end glaucoma screening, avoiding the training of intermediate models and manual annotation of ROI. This architecture is different from the existing mainstream MLP, CNN, Transformer and their hybrid models. The innovations of QAFIM are reflected in three aspects: dynamically generating complex anchors that integrate quantum-state parameterization and anchor adaptability; simulating quantum-state rotation and measurement through joint modulation of phase rotation and amplitude decay; achieving gauge-connection-driven feature aggregation to enhance geometric robustness by means of phase - difference calculation inspired by gauge field theory. Furthermore, the proposed Self-Interaction Module (SIM) can significantly enhance the adaptive generalization ability of multi-scale features with a relatively small number of parameters. Experiments on the AIROGS dataset show that the method achieves a sensitivity of 0.8594, a specificity of 95.86%, and an AUC of 0.9944, surpassing the best results (0.8533, 95%, 0.9898) of the AIROGS Challenge and the performance of most current advanced models. Moreover, it is close to the level of human medical experts (sensitivity 86%, specificity 94%) and has the potential to replace manual work in clinical settings. It also outperforms existing architectures in terms of computational efficiency (3.6G FLOPs) and the number of parameters (22,230,694). Meanwhile, a large number of model-comparison experiments on two external test sets, GAMMA and REFUGE, verify the effectiveness and generalization ability of the model.
